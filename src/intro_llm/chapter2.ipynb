{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t.taniguchi/Library/Caches/pypoetry/virtualenvs/study-lmm-wQFh0UWk-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTã‚’ä½¿ã£ã¦ã¿ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯å±±é ‚ã®ä¸‰è§’ç‚¹(30,062m)ã‹ã‚‰ 30.5ã‚­ãƒ­ã®æ‰€ã«ã‚ã‚‹ ç„¼å±±(ã‚„ã‘ã‚„ã¾) ã ãã†ã§ ä»Šå›ã¯ ç„¼å±±ã‚ˆã‚Šä¸Šã® å¤§å±±ã¯\n"
     ]
    }
   ],
   "source": [
    "# å¾Œç¶šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’äºˆæ¸¬ã™ã‚‹pipelineã‚’ä½œæˆ\n",
    "generator = pipeline(\n",
    "    \"text-generation\", model=\"abeja/gpt2-large-japanese\"\n",
    ")\n",
    "# \"æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯\"ã«ç¶šããƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ\n",
    "outputs = generator(\"æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯\")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTã‚’ä½¿ã£ã¦ã¿ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884172</td>\n",
       "      <td>12569</td>\n",
       "      <td>æ±äº¬</td>\n",
       "      <td>æ—¥æœ¬ ã® é¦–éƒ½ ã¯ æ±äº¬ ã§ ã‚ã‚‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024820</td>\n",
       "      <td>12759</td>\n",
       "      <td>å¤§é˜ª</td>\n",
       "      <td>æ—¥æœ¬ ã® é¦–éƒ½ ã¯ å¤§é˜ª ã§ ã‚ã‚‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020863</td>\n",
       "      <td>13017</td>\n",
       "      <td>äº¬éƒ½</td>\n",
       "      <td>æ—¥æœ¬ ã® é¦–éƒ½ ã¯ äº¬éƒ½ ã§ ã‚ã‚‹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str           sequence\n",
       "0  0.884172  12569        æ±äº¬  æ—¥æœ¬ ã® é¦–éƒ½ ã¯ æ±äº¬ ã§ ã‚ã‚‹\n",
       "1  0.024820  12759        å¤§é˜ª  æ—¥æœ¬ ã® é¦–éƒ½ ã¯ å¤§é˜ª ã§ ã‚ã‚‹\n",
       "2  0.020863  13017        äº¬éƒ½  æ—¥æœ¬ ã® é¦–éƒ½ ã¯ äº¬éƒ½ ã§ ã‚ã‚‹"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹pipelineã‚’ä½œæˆ\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\", model=\"cl-tohoku/bert-base-japanese-v3\"\n",
    ")\n",
    "masked_text = \"æ—¥æœ¬ã®é¦–éƒ½ã¯[MASK]ã§ã‚ã‚‹\"\n",
    "# [MASK]éƒ¨åˆ†ã‚’äºˆæ¸¬\n",
    "outputs = fill_mask(masked_text)\n",
    "# ä¸Šä½3ä»¶ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º\n",
    "display(pd.DataFrame(outputs[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683933</td>\n",
       "      <td>23845</td>\n",
       "      <td>ç´ æ™´ã‚‰ã—ã„</td>\n",
       "      <td>ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ ç´ æ™´ã‚‰ã—ã„ ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101235</td>\n",
       "      <td>24683</td>\n",
       "      <td>é¢ç™½ã„</td>\n",
       "      <td>ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ é¢ç™½ã„ ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048003</td>\n",
       "      <td>26840</td>\n",
       "      <td>æ¥½ã—ã„</td>\n",
       "      <td>ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ æ¥½ã—ã„ ã€‚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str                                   sequence\n",
       "0  0.683933  23845     ç´ æ™´ã‚‰ã—ã„  ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ ç´ æ™´ã‚‰ã—ã„ ã€‚\n",
       "1  0.101235  24683       é¢ç™½ã„    ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ é¢ç™½ã„ ã€‚\n",
       "2  0.048003  26840       æ¥½ã—ã„    ä»Šæ—¥ ã® æ˜ ç”» ã¯ åˆºæ¿€ çš„ ã§ é¢ç™½ã‹ã£ ãŸ ã€‚ ã“ã® æ˜ ç”» ã¯ æ¥½ã—ã„ ã€‚"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_text = \"ä»Šæ—¥ã®æ˜ ç”»ã¯åˆºæ¿€çš„ã§é¢ç™½ã‹ã£ãŸã€‚ã“ã®æ˜ ç”»ã¯[MASK]ã€‚\"\n",
    "outputs = fill_mask(masked_text)\n",
    "display(pd.DataFrame(outputs[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5ã‚’ä½¿ã£ã¦ã¿ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾³å·å®¶åº·\n"
     ]
    }
   ],
   "source": [
    "# text-to-textã§ç”Ÿæˆã™ã‚‹pipelineã‚’ä½œæˆ\n",
    "t2t_generator = pipeline(\n",
    "    \"text2text-generation\", model=\"retrieva-jp/t5-large-long\"\n",
    ")\n",
    "# ãƒã‚¹ã‚¯ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ³ã‚’äºˆæ¸¬\n",
    "masked_text = \"æ±Ÿæˆ¸å¹•åºœã‚’é–‹ã„ãŸã®ã¯ã€<extra_id_0>ã§ã‚ã‚‹\"\n",
    "outputs = t2t_generator(masked_text, eos_token_id=32098)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator.tokenizer.convert_tokens_to_ids(\"<extra_id_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—¥æœ¬éŠ€è¡Œ\n"
     ]
    }
   ],
   "source": [
    "masked_text = \"æ—¥æœ¬ã§é€šè²¨ã‚’ç™ºè¡Œã—ã¦ã„ã‚‹ã®ã¯ã€<extra_id_0>ã§ã‚ã‚‹\"\n",
    "outputs = t2t_generator(masked_text, eos_token_id=32098)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"æ—¥æœ¬éŠ€è¡Œ\" in t2t_generator.tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"æ—¥æœ¬éŠ€è¡Œ\"ã¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã®èªå½™ã«å«ã¾ã‚Œã¦ã„ãªã„ã®ã§é›£ã—ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = {\n",
    "    \"ãŸã®ã—ã„\": 6,\n",
    "    \"ãŸã®ã—ã•\": 2,\n",
    "    \"ã†ã¤ãã—ã„\": 4,\n",
    "    \"ã†ã¤ãã—ã•\": 1,\n",
    "}\n",
    "# èªå½™ã‚’æ–‡å­—ã§åˆæœŸåŒ–\n",
    "vocab = sorted(set([char for word in word_freqs for char in word]))\n",
    "# å˜èªã¨ãã®åˆ†å‰²çŠ¶æ…‹\n",
    "splits = {word: [char for char in word] for word in word_freqs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_most_frequent_pair(\n",
    "    splits: dict[str, list[str]]\n",
    "    ) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    æœ€ã‚‚é »åº¦ã®é«˜ã„éš£æ¥ã™ã‚‹ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    \"\"\"\n",
    "    pair_freqs = Counter()  # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã®ã‚«ã‚¦ãƒ³ã‚¿\n",
    "    for word, freq in word_freqs.items():  # ã™ã¹ã¦ã®å˜èªã‚’å‡¦ç†\n",
    "        split = splits[word]  # ç¾åœ¨ã®å˜èªã®åˆ†å‰²çŠ¶æ…‹ã‚’å–å¾—\n",
    "        # ã™ã¹ã¦ã®éš£æ¥ã—ãŸã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’å‡¦ç†\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã®é »åº¦ã«å˜èªã®é »åº¦ã‚’åŠ ç®—\n",
    "            pair_freqs[pair] += freq\n",
    "    # ã‚«ã‚¦ãƒ³ã‚¿ã‹ã‚‰æœ€ã‚‚é »åº¦ã®é«˜ã„ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’å–å¾—\n",
    "    pair, _ = pair_freqs.most_common(1)[0]\n",
    "    return pair\n",
    "\n",
    "def merge_pair(\n",
    "    target_pair: tuple[str, str], splits: dict[str, list[str]]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’çµåˆã™ã‚‹\n",
    "    \"\"\"\n",
    "    l_str, r_str = target_pair\n",
    "\n",
    "    for word in word_freqs:  # ã™ã¹ã¦ã®å˜èªã‚’å‡¦ç†\n",
    "        split = splits[word]  # ç¾åœ¨ã®å˜èªã®åˆ†å‰²çŠ¶æ…‹ã‚’å–å¾—\n",
    "        i = 0\n",
    "        # ã™ã¹ã¦ã®éš£æ¥ã—ãŸã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’å‡¦ç†\n",
    "        while i < len(split) - 1:\n",
    "            # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ãŒçµåˆå¯¾è±¡ã¨ä¸€è‡´ã—ãŸã‚‰çµåˆ\n",
    "            if split[i] == l_str and split[i + 1] == r_str:\n",
    "                split = split[:i] + [l_str + r_str] + split[i + 2 :]\n",
    "            i += 1\n",
    "        splits[word] = split  # ç¾åœ¨ã®çµåˆçŠ¶æ…‹ã‚’æ›´æ–°\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(9):\n",
    "    # æœ€ã‚‚é »åº¦ã®é«˜ã„éš£æ¥ã™ã‚‹ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’è¨ˆç®—\n",
    "    target_pair = compute_most_frequent_pair(splits)\n",
    "    # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’çµåˆ\n",
    "    splits = merge_pair(target_pair, splits)\n",
    "    # èªå½™ã«ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã‚’è¿½åŠ \n",
    "    vocab.append(target_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ã„', 'ã†', 'ã', 'ã•', 'ã—', 'ãŸ', 'ã¤', 'ã®', ('ã—', 'ã„'), ('ãŸ', 'ã®'), ('ãŸã®', 'ã—ã„'), ('ã†', 'ã¤'), ('ã†ã¤', 'ã'), ('ã†ã¤ã', 'ã—ã„'), ('ã—', 'ã•'), ('ãŸã®', 'ã—ã•'), ('ã†ã¤ã', 'ã—ã•')]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['è‡ªç„¶', 'è¨€èª', 'å‡¦ç†', 'ã«', 'ãƒ‡ã‚£ãƒ¼ãƒ—', 'ãƒ©ãƒ¼', '##ãƒ‹ãƒ³ã‚°', 'ã‚’', 'ä½¿ã†']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bert_ja_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-v3\"\n",
    "    )\n",
    "print(\n",
    "    bert_ja_tokenizer.tokenize(\"è‡ªç„¶è¨€èªå‡¦ç†ã«ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ã†\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-lmm-wQFh0UWk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
